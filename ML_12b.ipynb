{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes Theorem is based on conditional probability, which is the probability of one event occurring based on the the previous/known data about another event. Bayes Theorem can be interpreted mathematically as:\n",
    "\n",
    "$$\n",
    "   P(A | B) = \\frac{P(B | A)*P(A)}{P(A)}\n",
    "$$\n",
    "\n",
    "A, B       = Events\n",
    "\n",
    "P(A | B)     = Probability of A when B is known\n",
    "\n",
    "P(B | A)     = Probability of B when A is known\n",
    "\n",
    "P(A), P(B) = The independent probabilities of A and B\n",
    "\n",
    "Let's understand the Bayes theorem with following example:\n",
    "A  bookstore manager has information about his customers’ age and income. He wants to know how book sales are distributed across three age-classes of customers: youth (18-35), middle-aged (35-60), and seniors (60+). \n",
    "\n",
    "Let us call our known data as X. Our hypothesis is H, where we have some X that belongs to a certain class C. The goal is to determine the conditional probability of our hypothesis H given that X is present, i.e., P(H | X). In the layman term, for instance, a 26 years old with an income of $2000. H is our hypothesis that the customer will buy the book or no.\n",
    "\n",
    "Given these, Bayes Theorem states:\n",
    "\n",
    "$$\n",
    "   P(H | X) = \\frac{P(X | H)*P(H)}{P(X)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Naive Bayes is the application of Bayes Theorem for classification given the known features, therefore, it is called Naive Bayes claasifier. Naive Bayes has several methods of supervised learning algorithms based on applying Bayes’ theorem with the assumption that each pair of features is independent in itself given the value of the class variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications\n",
    "\n",
    "- Face Recognition\n",
    "- Weather Prediction\n",
    "- Medical Diagnosis\n",
    "- News Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages\n",
    "\n",
    "Following are some of the benefits of the Naive Bayes classifier: \n",
    "\n",
    "- It is simple and easy to implement\n",
    "- It doesn’t require as much training data\n",
    "- It handles both continuous and discrete data\n",
    "- It is highly scalable with the number of predictors and data points\n",
    "- It is fast and can be used to make real-time predictions\n",
    "- It is not sensitive to irrelevant features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disadvantages\n",
    "\n",
    "- If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as Zero Frequency.\n",
    "- On the other side naive Bayes is also known as a bad estimator, so the probability outputs are not to be taken too seriously.\n",
    "- Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods of Naive Bayes Classifier\n",
    "There are several methods to implement the Naive Bayes Classifier:\n",
    "1. Gaussian Naive Bayes\n",
    "2. Multinomial Naive Bayes\n",
    "3. Complement Naive Bayes\n",
    "4. Bernoulli Naive Bayes\n",
    "5. Categorical Naive Bayes\n",
    "6. Out-of-core naive Bayes model fitting\n",
    "\n",
    "For the sake of practice, we will use Gaussian Naive Bayes for implementation of Naive Bayes Classifier on the Wine Dataset from sklearn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 01-** Imort the required libraries and the dataset\n",
    "In this step, the Iris dataset is imported from the scikit-learn library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 02-** Select the features (X) and response variable (y) from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 03-** In this step, first of all train_test_split function is imported from the scikit-learn library. Next, the data is split into train data and test data with the proportion of 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 04-** GaussianNB Algorithm is imported from Naive Bayes window of Scikit-learn library, then the model is trained on the training data, where the variables X_train and y_train are fit to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 05-** Now is the time to put the test data inside the model for predictions upon that data provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 2, 1, 2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       2, 0, 2, 1, 0, 0, 1, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = gnb.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 06-** In the end, the performance of trained model is evaluated by importing the metrics function from scikit-learn library. Next, the accuracy_score function is uses to see the score of our model. While evaluating the model, the actual output values (y_test) are compared with the predicted values by the model, which we assigned to y_pred in the above step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes model accuracy(in %): 96.66666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Gaussian Naive Bayes model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "1) https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c\n",
    "2) https://www.geeksforgeeks.org/naive-bayes-classifiers/"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "35d50e7596a91d38978837c1c880a54ccfdd350a17a0f76616c06fe3c2115e36"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
